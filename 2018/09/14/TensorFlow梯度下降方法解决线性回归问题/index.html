<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="孙远博, sybapp@qq.com"><title>TensorFlow梯度下降方法解决线性回归问题 · 不将就 | 一个计算机小白的博客</title><meta name="description" content="TensorFlow 梯度下降方法解决线性回归问题这是 TensorFlow 最简单的问题了，入门级的。
1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253"><meta name="keywords" content="Hexo, HTML, CSS, Android, Linux, WEB"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title><a href="/">不将就 | 一个计算机小白的博客</a></h3><div class="description"><p>一个计算机小白的博客 C / Python / Android / Linux / WEB</p></div></div></div><ul class="social-links"><li><a href="http://weibo.com/ansgsy"><i class="fa fa-weibo"></i></a></li><li><a href="http://github.com/sybapp"><i class="fa fa-github"></i></a></li></ul><div class="footer"><div class="by_farbox"><a href="https://sybapp.tk/" target="_blank">&#169 2018 不将就 All rights reserved. &#160;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li><li><a href="/links">友链</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>TensorFlow梯度下降方法解决线性回归问题</a></h3></div><div class="post-content"><h1 id="TensorFlow-梯度下降方法解决线性回归问题"><a href="#TensorFlow-梯度下降方法解决线性回归问题" class="headerlink" title="TensorFlow 梯度下降方法解决线性回归问题"></a>TensorFlow 梯度下降方法解决线性回归问题</h1><p>这是 TensorFlow 最简单的问题了，入门级的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">使用梯度下降的优化方法快速解决线性回归问题</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造输入数据</span></span><br><span class="line"><span class="comment"># 数据数量</span></span><br><span class="line">points_num = <span class="number">1000</span></span><br><span class="line">vectors = []</span><br><span class="line"><span class="comment"># 使用 Numpy 的正态分布函数生成 100 个点</span></span><br><span class="line"><span class="comment"># 这些点的 (x, y) 的坐标值对应线性方程 y = 0.1 * x + 0.2</span></span><br><span class="line"><span class="comment"># 权重 (Weight) 0.1，偏差 (Bias) 0.2</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(points_num):</span><br><span class="line">    x1 = np.random.normal(<span class="number">0.0</span>, <span class="number">1.0</span>)</span><br><span class="line">    y1 = <span class="number">0.1</span> * x1 + <span class="number">0.2</span> + np.random.normal(<span class="number">0.0</span>, <span class="number">0.05</span>)</span><br><span class="line">    vectors.append([x1, y1])</span><br><span class="line"></span><br><span class="line">x_data = [v[<span class="number">0</span>] <span class="keyword">for</span> v <span class="keyword">in</span> vectors]</span><br><span class="line">y_data = [v[<span class="number">1</span>] <span class="keyword">for</span> v <span class="keyword">in</span> vectors]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建线性回归模型</span></span><br><span class="line"><span class="comment"># 初始化 Weight 和 bias</span></span><br><span class="line">w = tf.Variable(tf.random_uniform([<span class="number">1</span>], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line"><span class="comment"># 模型计算出来的 y</span></span><br><span class="line">y = w * x_data + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数 (loss function)</span></span><br><span class="line"><span class="comment"># 求其方差</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y - y_data))</span><br><span class="line"><span class="comment"># 使用优化器优化 loss function</span></span><br><span class="line"><span class="comment"># 设置学习率</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.05</span>)</span><br><span class="line"><span class="comment"># 训练使其方差最小</span></span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 初始化所有 tensor</span></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    steps = <span class="number">100</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(steps):</span><br><span class="line">        sess.run(train)</span><br><span class="line">        print(<span class="string">'Step = %d, Loss = %f\n'</span></span><br><span class="line">              <span class="string">'[Weight = %f, bias = %f]'</span> % (step, sess.run(loss), sess.run(w), sess.run(b)))</span><br><span class="line"></span><br><span class="line">    plt.plot(x_data, y_data, <span class="string">'r.'</span>, label=<span class="string">'Original data'</span>)</span><br><span class="line">    plt.title(<span class="string">'Linear Regression using Gradient Descent'</span>)</span><br><span class="line">    <span class="comment"># 训练出的拟合的一条曲线</span></span><br><span class="line">    plt.plot(x_data, sess.run(w) * x_data + sess.run(b), <span class="string">'b-'</span>, label=<span class="string">'Fitted line'</span>, )</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'y'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>结果如图：</p>
<p><img src="https://i.loli.net/2018/09/14/5b9b59525ee69.png" alt></p>
<p><img src="https://i.loli.net/2018/09/14/5b9b5acf81c9b.png" alt><br>已经非常接近真实值了</p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2018-09-14</span><i class="fa fa-tag"></i><a class="tag" href="/tags/机器学习/" title="机器学习">机器学习 </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,https://sybapp.tk/2018/09/14/TensorFlow梯度下降方法解决线性回归问题/,不将就 | 一个计算机小白的博客,TensorFlow梯度下降方法解决线性回归问题,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2018/09/15/拥抱人工智能，从机器学习开始/" title="拥抱人工智能，从机器学习开始">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2018/08/13/flask仿豆瓣微信小程序/" title="flask仿豆瓣微信小程序">下一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>